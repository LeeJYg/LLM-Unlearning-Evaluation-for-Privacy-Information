# LLM-Unlearning-for-Privacy-Information
This repository is dedicated to exploring **LLM unlearning**, focusing on privacy risks and strategies to effectively and efficiently "forget" specific data from large language models. 

---

## Key Topics
### Privacy Attack
- **Membership Inference Attacks (MIA):** Identifying training data.
- **Data Reconstruction Attacks:** Extracting sensitive content.
- **Privacy Regulation Compliance:** Addressing data deletion requests (e.g., GDPR).

### Unlearning Techniques
- **Fine-Tuning Adjustments:** Retraining on modified datasets to erase sensitive data.
- **Selective Forgetting:** Targeted removal of data contributions.
- **Utility Preservation:** Maintaining overall model functionality.

### Existing Unlearning Evaluation Framework
- **Unlearning Effectiveness:** Assessing removal completeness.
- **Privacy Metrics:** Measuring resilience against adversarial attacks.
- **Model Utility:** Evaluating performance trade-offs in downstream tasks.

---

## Future Directions
- Developing faster and more effective unlearning algorithms.
- Establishing standard benchmarks for unlearning evaluation.
